{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3eb1YScNIIphklG20Xlfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaaat/Machine_learning-Deep_learning/blob/main/End_to_End_Text_Classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "import shutil\n",
        "import string\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "dEImrQUOtvbY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let is download and fetchh the database\n",
        "dataset_link = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset = tf.keras.utils.get_file(fname = \"aclImdb_v1\", origin = dataset_link, untar = True, cache_dir = \".\", cache_subdir = \" \")\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVGUj_fN4HAp",
        "outputId": "17cb65e0-262f-44eb-e766-3c061665bdd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 12s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtWEKBDi8xIB",
        "outputId": "8ca60874-a728-4391-85c3-064fe5bd3c0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'README', 'imdbEr.txt', 'imdb.vocab', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To get the train_dir...\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2keprouB4Dw",
        "outputId": "71a7701e-545c-4cda-9d7a-7f89c92a1ead"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'pos',\n",
              " 'unsup',\n",
              " 'neg',\n",
              " 'unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.listdir(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51YsaykqCfzn",
        "outputId": "bd0a447c-ce17-400f-c2f3-3a52f483deb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos', 'neg', 'urls_pos.txt', 'urls_neg.txt', 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us fetch a sample file in the train set\n",
        "sample_file = os.path.join(train_dir, \"pos/1181_9.txt\")\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxO7VyowCqky",
        "outputId": "06bdcd07-b81c-46ac-c392-fce023308063"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj33k5iBGADV",
        "outputId": "cdb866c4-59bf-4240-da7e-8823e6ac8b5e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'pos',\n",
              " 'unsup',\n",
              " 'neg',\n",
              " 'unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_dir = os.path.join(train_dir, \"unsup\")\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "ko29ireUGDf6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdGsR9F4GYfw",
        "outputId": "a9b3b799-f22f-4c74-a569-16ec6ee6f3a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'pos',\n",
              " 'neg',\n",
              " 'unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ceate a validation set\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "raw_training_dataset = tf.keras.utils.text_dataset_from_directory(train_dir, batch_size = batch_size, seed = seed, shuffle = True, validation_split=0.2, subset = \"training\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPa10SwfGbxG",
        "outputId": "eb5e9755-55a9-4227-c3ea-98422dd11125"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_validation_dataset = tf.keras.utils.text_dataset_from_directory(train_dir,\n",
        "                                                                    batch_size = batch_size,\n",
        "                                                                    seed = seed,\n",
        "                                                                    shuffle = True,\n",
        "                                                                    validation_split = 0.2,\n",
        "                                                                    subset  = \"validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJPDJ7E7IVyf",
        "outputId": "3fd1dbb9-8062-4316-f63d-86933c211704"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us print out a few samples of our training samples\n",
        "for sample_train_batch, label_batch in raw_training_dataset.take(1):\n",
        "  for i in range(5):\n",
        "    print(f\"Review: {sample_train_batch.numpy()[i]}\")\n",
        "    print(f\"Label: {label_batch.numpy()[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LVPuoCtIxdu",
        "outputId": "ef2b3ed8-933d-468e-e1b0-766cdc4d4076"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.'\n",
            "Label: 0\n",
            "Review: b\"I am shocked. Shocked and dismayed that the 428 of you IMDB users who voted before me have not given this film a rating of higher than 7. 7?!?? - that's a C!. If I could give FOBH a 20, I'd gladly do it. This film ranks high atop the pantheon of modern comedy, alongside Half Baked and Mallrats, as one of the most hilarious films of all time. If you know _anything_ about rap music - YOU MUST SEE THIS!! If you know nothing about rap music - learn something!, and then see this! Comparisons to 'Spinal Tap' fail to appreciate the inspired genius of this unique film. If you liked Bob Roberts, you'll love this. Watch it and vote it a 10!\"\n",
            "Label: 1\n",
            "Review: b'What a lovely heart warming television movie. The story tells of a little five year old girl who has lost her daddy and finds it impossible to cope. Her mother is also very distressed ..only a miracle can alleviate their unhappiness.Which all viewers hope will materialise. Samantha Mathis is brilliant as the little girl\\'s mum ,as she was as the nanny in\" Jack and Sarah\",worth watching if you like both Samantha Mathis and happy; year tear jerking movies! Ellen Burstyn is, as, always a delightful grandmother in this tender and magnificently acted movie. Jodelle Ferland (the little five year old) is charming and a most convincing young actress. The film is based on a true story which makes it so touching.\"Mermaid\" is a tribute to the milk of human kindness which is clearly illustrated and clearly is still all around us in this difficult world we live in. \"Mermaid\" gives us all hope ,by realising that there a lot of lovely people in the world with lot\\'s of love to give. James Robson Glasgow Scotland U.K.'\n",
            "Label: 1\n",
            "Review: b'I haven\\'t read this book, but all through the movie I was awestruck with only one thought in my head: This is so Vonnegut. I have never seen an author, all of the intelligence and life behind the workings of a novel, translated so well to film. This movie had the same complexities found in Vonnegut\\'s novels: the jokes were often meaningful and symbolic, and the dramatic events and symbols were often also jokes.<br /><br />Campbell was also a very Vonnegut character, portrayed perfectly by Nick Nolte. He had all of the earmarks of a Vonnegut \"hero\": lack of concern for political boundaries, ironic dark humor giving way to dumb inactivity in response to stress, and an unwillingness to push his version of reality on those around him.<br /><br />Overall, I was constantly surprised and impressed as I watched this movie. It was the same feeling I had reading \"Cat\\'s Cradle,\" my first Vonnegut novel, as if the most perfectly oddball thing that could happen, he thought of THAT, and he made it real and important. Yes, he has nothing but army surplus \"White Christmas\" albums. So it goes!'\n",
            "Label: 1\n",
            "Review: b'\"A Cry in the Dark\" is a masterful piece of cinema, haunting, and incredibly though provoking. The true story of Lindy Chamberland, who, in 1980, witnessed a horrific sight, seeing her 3-month-old baby being brutally taken from their family\\'s tent, while camping on the Austrailian outback. Azaria (the baby) was never seen again, and the result of her horrendous disappearance caused a true life frenzy all around the world. Meryl Streep does immaculate justice to the role of Lindy, as she always does. But the one thing that helps \"A Cry in the Dark\" never fall flat is the brilliant direction. A truly inspired and accurate outlook on this baffeling case, tears are brought to the eyes. The concept is nothing less then terrifying, and afterwards you are left haunted, but also inspired.'\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_training_dataset.class_names[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2nSS5nZ4MwHQ",
        "outputId": "8b6d334d-fae0-4c81-aa74-7ee0f2247b6b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_testing_dataset = tf.keras.utils.text_dataset_from_directory(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z7dE9kEOMJw",
        "outputId": "cce6f9a8-f250-4be1-a00c-2e2f45539d13"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data for training"
      ],
      "metadata": {
        "id": "r6eIDSaBOsjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This will largely involve standardize, tokenize, and vectorize the data using the helpful tf.keras.layers.TextVectorization layer. \"\"\"\n",
        "#Ceate a custom standardizer\n",
        "def custom_standardizer(input_data):\n",
        "  #Convert to lower case\n",
        "  lower_case = tf.strings.lower(input_data)\n",
        "  #Strip off the html tags and space\n",
        "  stripped_html = tf.strings.regex_replace(lower_case, \"<br />\", \" \")\n",
        "  return tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(string.punctuation), \" \")"
      ],
      "metadata": {
        "id": "ZNpoKXoyOgoC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will have vectorizing layer\n",
        "maximum_features = 10000\n",
        "sequence_length = 250\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens = maximum_features,\n",
        "                                                    standardize = custom_standardizer,\n",
        "                                                    output_mode = \"int\",\n",
        "                                                    output_sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "DA-qZbDFTmAv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRTbfiFXXlCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The next stage will require us to make a text-only dataset (without labels), then call adapt"
      ],
      "metadata": {
        "id": "jMrCaaM6X6LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_text = raw_training_dataset.map(lambda x, y:x)\n",
        "vectorize_layer.adapt(training_text)"
      ],
      "metadata": {
        "id": "6XApk1kmQZYs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us check the results of using this vectorize thing\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "_vEpV7wiZISx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_batch, label_batch = next(iter(raw_training_dataset))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(f\"Review: {first_review}\")\n",
        "print(f\"Label: {raw_training_dataset.class_names[first_label]}\")\n",
        "print(f\"Vectorized_text: {vectorize_text(first_review, first_label)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrI78O3TZ6Hq",
        "outputId": "caa0ae43-1375-4da3-dcc8-d14b7053815d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'\"A young woman suffers from the delusion that she is a werewolf, based upon a family legend of an ancestor accused of and killed for allegedly being one. Due to her past treatment by men, she travels the countryside seducing and killing the men she meets. Falling in love with a kind man, her life appears to take a turn for the better when she is raped and her lover is killed by a band of thugs. Traumatized again by these latest events, the woman returns to her violent ways and seeks revenge on the thugs,\" according to the DVD sleeve\\'s synopsis.<br /><br />Rino Di Silvestro\\'s \"La lupa mannara\" begins with full frontal, writhing, moaning dance by shapely blonde Annik Borel, who (as Daniella Neseri) mistakenly believes she is a werewolf. The hottest part is when the camera catches background fire between her legs. The opening \"flashback\" reveals her hairy ancestor was (probably) a lycanthropic creature. Ms. Borel is, unfortunately, not a werewolf; she is merely a very strong lunatic.<br /><br />As a film, \"Werewolf Woman\" (in English) would have been better if Borel\\'s character really was a female werewolf; with her sexual victimization a great bit of characterization. But, as far as 1970s skin and blood flicks go, this one is hard to beat. Bouncy Borel is either nude or sexily clad throughout the film, which features a fair amount of gratuitous gore. Dazzling Dagmar Lassander (as Elena) and hunky Howard Ross (as Luca) are good supporting players.'\n",
            "Label: neg\n",
            "Vectorized_text: (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[   4,  184,  243, 2415,   38,    2,    1,   12,   54,    7,    4,\n",
            "        1818,  439,  721,    4,  210, 1753,    5,   35,    1, 3489,    5,\n",
            "           3,  554,   16, 9084,  113,   30,  697,    6,   41,  502, 2284,\n",
            "          34,  341,   54, 4229,    2, 4310,    1,    3,  847,    2,  341,\n",
            "          54,  878, 1420,    9,  115,   18,    4,  242,  123,   41,  109,\n",
            "         740,    6,  195,    4,  460,   16,    2,  128,   53,   54,    7,\n",
            "        3398,    3,   41, 1406,    7,  554,   34,    4, 1063,    5, 3669,\n",
            "        9112,  172,   34,  132, 2448,  663,    2,  243, 1780,    6,   41,\n",
            "        1102,  776,    3, 5140, 1066,   22,    2, 3669, 1731,    6,    2,\n",
            "         278, 8919,   13, 3766,    1, 9745,    1,   13, 1072,    1,    1,\n",
            "         787,   18,  364, 6679,    1,    1,  817,   34,    1, 1944,    1,\n",
            "           1,   36,   15,    1,    1,    1, 2198,   54,    7,    4, 1818,\n",
            "           2,    1,  175,    7,   53,    2,  362, 4016,  991,  949,  193,\n",
            "          41, 3051,    2,  633, 2816, 2757,   41, 8228,    1,   14,  241,\n",
            "           4,    1, 1573, 1520,    1,    7,  467,   24,    4, 1818,   54,\n",
            "           7, 1569,    4,   55,  556, 6765,   15,    4,   20, 1818,  243,\n",
            "           9,  627,   61,   28,   77,  128,   47,    1,   13,  104,   65,\n",
            "          14,    4,  638, 1818,   18,   41,  845,    1,    4,   85,  225,\n",
            "           5, 3482,   19,   15,  228,   15, 3372, 2544,    3,  522, 1481,\n",
            "         137,   11,   30,    7,  252,    6, 1617,    1,    1,    7,  342,\n",
            "        2652,   42,    1, 5701,  472,    2,   20,   62,  928,    4, 1257,\n",
            "        1167,    5, 2213,  571, 6019,    1,    1,   15,    1,    3, 8409,\n",
            "        2145, 5494,   15,    1,   26,   50,  698, 1856]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can lookup the token (string) that each integer corresponds to by calling .get_vocabulary() on the layer.\n",
        "print(f\"1420: {vectorize_layer.get_vocabulary()[1420]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B_NjtY_cgSt",
        "outputId": "d3688013-b8ef-4971-f09b-3a83714bdc81"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1420: falling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We are nearly ready to train our model. As a final preprocessing step, we will apply the TextVectorization layer we created earlier to the train, validation, and test dataset."
      ],
      "metadata": {
        "id": "oQpPgSjZeVYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = raw_training_dataset.map(vectorize_text)\n",
        "validation_set = raw_validation_dataset.map(vectorize_text)\n",
        "testing_set = raw_testing_dataset.map(vectorize_text)"
      ],
      "metadata": {
        "id": "Xufy-ahneKvM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = training_set.cache().prefetch(buffer_size =AUTOTUNE )\n",
        "validation_ds = validation_set.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = testing_set.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "metadata": {
        "id": "SeDCGqKJeX8q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8RsQel_WhKN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Juicy Part -- Creating our Neural Network"
      ],
      "metadata": {
        "id": "euXUrIAJngdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16"
      ],
      "metadata": {
        "id": "YnsgVYtcn2_s"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(maximum_features, embedding_dim),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIY_bvnwnkXI",
        "outputId": "ddb46374-0ed9-487f-ff42-f7d8aa8abbbb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160000    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160017 (625.07 KB)\n",
            "Trainable params: 160017 (625.07 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function and optimizer A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), you'll use losses.BinaryCrossentropy loss function."
      ],
      "metadata": {
        "id": "z5Gpob3ip-bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = losses.BinaryCrossentropy(),\n",
        "              optimizer = \"adam\",\n",
        "              metrics = [tf.metrics.BinaryAccuracy(threshold = 0.5)])"
      ],
      "metadata": {
        "id": "RK5qWkUXqBOL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufgwlPd2qir4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let us now Train the model"
      ],
      "metadata": {
        "id": "3b2tukBhqlt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2TiUdEZqnsa",
        "outputId": "e8b39262-fe9b-4057-ec15-fdc8e9a042e4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 14ms/step - loss: 0.6626 - binary_accuracy: 0.6919 - val_loss: 0.6131 - val_binary_accuracy: 0.7670\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.5469 - binary_accuracy: 0.8002 - val_loss: 0.4972 - val_binary_accuracy: 0.8182\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.4430 - binary_accuracy: 0.8459 - val_loss: 0.4192 - val_binary_accuracy: 0.8472\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3762 - binary_accuracy: 0.8664 - val_loss: 0.3735 - val_binary_accuracy: 0.8590\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3329 - binary_accuracy: 0.8813 - val_loss: 0.3446 - val_binary_accuracy: 0.8678\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3021 - binary_accuracy: 0.8917 - val_loss: 0.3258 - val_binary_accuracy: 0.8726\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2789 - binary_accuracy: 0.8998 - val_loss: 0.3127 - val_binary_accuracy: 0.8742\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2605 - binary_accuracy: 0.9064 - val_loss: 0.3031 - val_binary_accuracy: 0.8780\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2442 - binary_accuracy: 0.9117 - val_loss: 0.2963 - val_binary_accuracy: 0.8774\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2294 - binary_accuracy: 0.9173 - val_loss: 0.2920 - val_binary_accuracy: 0.8794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  print(epoch)\n",
        "  print(model.fit(train_ds, validation_data=validation_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnfycYOEsSEx",
        "outputId": "c0223c94-edaf-419e-b5df-f39695785118"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2166 - binary_accuracy: 0.9229 - val_loss: 0.2890 - val_binary_accuracy: 0.8808\n",
            "<keras.src.callbacks.History object at 0x7d5071755ba0>\n",
            "1\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2063 - binary_accuracy: 0.9263 - val_loss: 0.2874 - val_binary_accuracy: 0.8826\n",
            "<keras.src.callbacks.History object at 0x7d506e1713c0>\n",
            "2\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1958 - binary_accuracy: 0.9323 - val_loss: 0.2869 - val_binary_accuracy: 0.8814\n",
            "<keras.src.callbacks.History object at 0x7d506ff255d0>\n",
            "3\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1852 - binary_accuracy: 0.9348 - val_loss: 0.2872 - val_binary_accuracy: 0.8818\n",
            "<keras.src.callbacks.History object at 0x7d506ff27a00>\n",
            "4\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1768 - binary_accuracy: 0.9398 - val_loss: 0.2887 - val_binary_accuracy: 0.8812\n",
            "<keras.src.callbacks.History object at 0x7d507172f310>\n",
            "5\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1687 - binary_accuracy: 0.9421 - val_loss: 0.2904 - val_binary_accuracy: 0.8808\n",
            "<keras.src.callbacks.History object at 0x7d507154e170>\n",
            "6\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1610 - binary_accuracy: 0.9463 - val_loss: 0.2925 - val_binary_accuracy: 0.8808\n",
            "<keras.src.callbacks.History object at 0x7d507172f3a0>\n",
            "7\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1549 - binary_accuracy: 0.9486 - val_loss: 0.2955 - val_binary_accuracy: 0.8800\n",
            "<keras.src.callbacks.History object at 0x7d506ff24430>\n",
            "8\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1478 - binary_accuracy: 0.9520 - val_loss: 0.2992 - val_binary_accuracy: 0.8804\n",
            "<keras.src.callbacks.History object at 0x7d507154d8d0>\n",
            "9\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1409 - binary_accuracy: 0.9546 - val_loss: 0.3026 - val_binary_accuracy: 0.8796\n",
            "<keras.src.callbacks.History object at 0x7d506ff27310>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us evaluatloss\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qau5cseMtGXU",
        "outputId": "10e2ddbf-f9e2-418b-c73c-f7e318c57f01"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 17ms/step - loss: 0.3389 - binary_accuracy: 0.8649\n",
            "Loss: 0.3388782739639282\n",
            "Accuracy: 0.8648800253868103\n"
          ]
        }
      ]
    }
  ]
}